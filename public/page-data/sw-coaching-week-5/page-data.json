{"componentChunkName":"component---src-templates-post-template-tsx","path":"/sw-coaching-week-5/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"html":"<p><em>본 포스트는 IOWA 대학 <strong>이강표</strong>(Kang-Pyo Lee) 박사님의 허락을 구하고 강의를 정리한 것입니다.</em><br>\n<em>강의 사진, 코드의 저작권은 모두 이강표 박사님께 있습니다.</em></p>\n<h2 id=\"1-강의-정리\" style=\"position:relative;\"><a href=\"#1-%EA%B0%95%EC%9D%98-%EC%A0%95%EB%A6%AC\" aria-label=\"1 강의 정리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 강의 정리</h2>\n<h3 id=\"k-means-clustering\" style=\"position:relative;\"><a href=\"#k-means-clustering\" aria-label=\"k means clustering permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>k-Means Clustering</h3>\n<p><strong>특징:</strong></p>\n<ul>\n<li>simplest, most commonly used</li>\n<li>데이터에서 특정 영역을 대표하는 centroids 를 활용.</li>\n</ul>\n<p><br>\n<strong>과정:</strong></p>\n<ol>\n<li>K개의 cluster에 대한 K개의 centroids 들을 랜덤 하게 설정</li>\n<li>다음의 3,4번을 반복</li>\n<li>각 데이터 포인트에 대해 centroids 사이의 거리를 계산하여 가장 가까운 centroid에 할당.</li>\n<li>같이 할당된 데이터 포인트들 사이에서 평균을 구해 centroid 를 업데이트.</li>\n<li>더 이상 centroids 가 변하지 않을 경우 종료.</li>\n</ol>\n<p><br>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 359px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/5ffa9a7509655738d843964cf0941108/16540/1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 66.66666666666666%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAB6ElEQVQ4y21Ta4/aMBDM//+DbZFycJBw16sIcMFe78Oeajch9EORLCKGnew83HFhmBlM/ShEBcKCWiuYGVYrqlWoafxPZMFKKcucGVQ1jmOdA/5ptaHXHm/6BpVl2LEmitoqdrrDyU4wsQ2rqkBT/Pl1xjgy1ASdb9haAxuDjDDZhMw5tmIn9DdXAVXCVC9g4cBiQ1VUZjAxrjcFEaMzNnzaJ97lPaT4piHV5ThWf+MgB6ABzG6DoJqB1SDjgDKeoRUQscBC8oeeMdsMVLy8MwMVCuxRH0Go+vK15AwazxBi1NZgtmBB6DJSTiHDjQ1/VuM9pEwJlCmM96EFo3gmKshEMReEIbM15JRQmD0ePH97fhciEOX/Yk5ElDasI1rebKbgTHjMDOYSEl6YISU3XSDCG+ZkPvd4CFKSmOsiyShOxbU/Y98nqEqQcGwMGIDdjnA8chBslXKsAT9+ZhxPjnko7gkL9H6DaMN0FeTsPhnYS+xb3G8RyDRpJBmEHo6flOJS3O9rbdjbPwygwwFSl02fKTvGw4Cy36O1AFeZXhtF6XvwOESN3MM1ZQJfJmiRRfg/V87D0MsFbZXuHQ3CtTby9RXFXrC6XD0fFlXM399IKW/xP4k9FG9ASmm7r69KKShnzPO81eYvor3301BtCXoAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/5ffa9a7509655738d843964cf0941108/a59e9/1.webp 192w,\n/static/5ffa9a7509655738d843964cf0941108/e956e/1.webp 359w\"\n              sizes=\"(max-width: 359px) 100vw, 359px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/5ffa9a7509655738d843964cf0941108/3b721/1.png 192w,\n/static/5ffa9a7509655738d843964cf0941108/16540/1.png 359w\"\n            sizes=\"(max-width: 359px) 100vw, 359px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/5ffa9a7509655738d843964cf0941108/16540/1.png\"\n            alt=\"1\"\n            title=\"1\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span><br>\n<br>\n<strong>장점:</strong></p>\n<ul>\n<li>상대적으로 이해하고 적용하기 쉬움.</li>\n<li>처리 시간이 짧음.</li>\n<li>큰 데이터에도 사용이 용이.</li>\n</ul>\n<p><strong>단점:</strong></p>\n<ul>\n<li>random initialization 에 의존하게 된다.</li>\n<li>간단한 형태의 cluster 만 캡쳐가 가능하다.</li>\n<li>cluster 에 대해 all directions 의 important 똑같다. 즉, 가중치가 동일하다.</li>\n<li>적절한 클러스터 개수 k 를 사용자가 직접 정해줘야 한다.</li>\n</ul>\n<h3 id=\"topic-modeling\" style=\"position:relative;\"><a href=\"#topic-modeling\" aria-label=\"topic modeling permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Topic Modeling</h3>\n<p><strong>개념:</strong></p>\n<ul>\n<li>abstract(추상적), latent(숨겨진) 토픽을 찾아내서 model 제작. 이를 바탕으로 document 분류.</li>\n<li>Latent Dirichlet Allocation (LDA)가 좋은 예시.</li>\n</ul>\n<p><br>\n<strong>Latent Dirichlet Allocation (LDA)핵심</strong></p>\n<ul>\n<li>document는 특정 토픽 내의 단어들을 조합하여 만들어진다고 가정.</li>\n<li>topics 를 찾기 위해 document, words 를 reverse engineers 한다.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 387px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/e0caa3ead4a74311eaf32fbaae61caf0/0082d/2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 45.833333333333336%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABtUlEQVQozz2Si27bIBRA8/+/NmlV125t2iTG2AZsMBgbx0ma9UxQrUhI1n0czgXvtvNGShvndGE7XzmvF66XD7QeCGEir/8x7yPBT3z+/WRdNpZ5JU4LTW0Qp5aHn7/YLXFFdxZnQ9mqG5CVpqk1U1iIMdEbh3M51zO6qYCMzj0e5zwvzyeqQ0t1bNnlZPXe0ghF1xj2vwUPP/Yc9w2jnWiE4fnxjf2fE08Pe7pmYF23Yr8sC+s5cTpIdNcz9CO7PGY2MMrSSsPLU4UUBq2GYph3NsnFQ+/wLpYxvQ/M81zAb6+CrjX02rG7bFdGFwpUNQON0GhlS8wNGeRLY3Xs8GPA9gGjxwJKaaGVPc+PB7Tqv4D3j3sxyMl1TYzOo9oBO4ysaWNNl+/mkK2mXHf+Nszx47sshmXkOM302jLPkRhjGSufmoHBx1I0hankMiSMsdjn7xzL0PdXQSPVF1DWshg5G3F2wqiRTlp64wtciIbRzYwuw0O5Q60Mdgjl0fKri6Oikz29cuzyf5ZS4nA40LYtxmjqui4n59V1HUKIEtPaIGrB7XbDWktVVSil0FohG8n9fucf+tCjDkjwnDkAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/e0caa3ead4a74311eaf32fbaae61caf0/a59e9/2.webp 192w,\n/static/e0caa3ead4a74311eaf32fbaae61caf0/0ca9f/2.webp 384w,\n/static/e0caa3ead4a74311eaf32fbaae61caf0/95960/2.webp 387w\"\n              sizes=\"(max-width: 387px) 100vw, 387px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/e0caa3ead4a74311eaf32fbaae61caf0/3b721/2.png 192w,\n/static/e0caa3ead4a74311eaf32fbaae61caf0/66595/2.png 384w,\n/static/e0caa3ead4a74311eaf32fbaae61caf0/0082d/2.png 387w\"\n            sizes=\"(max-width: 387px) 100vw, 387px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/e0caa3ead4a74311eaf32fbaae61caf0/0082d/2.png\"\n            alt=\"2\"\n            title=\"2\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<ul>\n<li>document, words matrix 에 topics 라는 새로운 축을 추가해 decompose 한다.<br>\n= singular valuede composition (SVD)</li>\n</ul>\n<h3 id=\"tip\" style=\"position:relative;\"><a href=\"#tip\" aria-label=\"tip permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>⭐TIP!⭐</h3>\n<p>Topic Modeling 과 k-Means Clustering 모두 <strong>Unsupervised Learning</strong> 임!<br>\n<br>\n문서 클러스터링 및 토픽 모델링 비교\r\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 768px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/67868a88a6d1c7dcd9604fdcb227ce9a/22121/3.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 45.833333333333336%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABkklEQVQoz1VR266bMBDk/7+qUqs+tOeoD1UCGExIACU4kBwFsI1vU3kTItWStdJ6dmZ2nIQQ8L52RXCGrrcawTv4ELAd7/0b642Cdxb/zYeAhB69QxyzIoOpf8C2n7DdJ/wqiWhZFvRCvEifZKb6Btv9gQ9PoU2MCPFy4acetvkF1/6G6z7g9Ux9uSy4Xq9vp+7yF+b4E67+Dj/yZ3NzaK2FMQbrusLqBU7PUEqSq/gWlbdDOGOglSQxJSWklNBaQykF5xyStm2R5zl4VcHcG/ivI+63G/peEMGW3TzP4GWJLGPouyPwdcBtEBDXAeMw0AbRQNIRIUNRFBjGEZdeENnlcoEQAsMwUCJSLijLksRPTYPxdkff94SLNWJjTZrTCfv9DgVj4LyiejgckDOGx+NBq8RstFJElmUZdrsdGGMoi4KM1HUNzjmmeUZyPp+JaGvW9ZHym6YJ8zxRPpEwOoyYqqoIFwmjuw2r15XioZX3aUpK7OVuHMf3unGAflpK5Hn2zJtzpGlKK8bsIvYxTUT4D+Bfq1ZHtKWsAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/67868a88a6d1c7dcd9604fdcb227ce9a/a59e9/3.webp 192w,\n/static/67868a88a6d1c7dcd9604fdcb227ce9a/0ca9f/3.webp 384w,\n/static/67868a88a6d1c7dcd9604fdcb227ce9a/dc9b9/3.webp 768w,\n/static/67868a88a6d1c7dcd9604fdcb227ce9a/0a256/3.webp 947w\"\n              sizes=\"(max-width: 768px) 100vw, 768px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/67868a88a6d1c7dcd9604fdcb227ce9a/3b721/3.png 192w,\n/static/67868a88a6d1c7dcd9604fdcb227ce9a/66595/3.png 384w,\n/static/67868a88a6d1c7dcd9604fdcb227ce9a/fe486/3.png 768w,\n/static/67868a88a6d1c7dcd9604fdcb227ce9a/22121/3.png 947w\"\n            sizes=\"(max-width: 768px) 100vw, 768px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/67868a88a6d1c7dcd9604fdcb227ce9a/fe486/3.png\"\n            alt=\"3\"\n            title=\"3\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<h2 id=\"2-코드-코칭\" style=\"position:relative;\"><a href=\"#2-%EC%BD%94%EB%93%9C-%EC%BD%94%EC%B9%AD\" aria-label=\"2 코드 코칭 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. 코드 코칭</h2>\n<p><em>코드 전문은 분량상 제외하였으며, 피드백 위주로 정리하였습니다. Colab 환경을 기반으로 합니다.</em><br>\n<br>\n이번 코드 코칭은 피드백보다는 <strong>설명 위주</strong>로 진행하였습니다.</p>\n<h3 id=\"tf-idf-활용\" style=\"position:relative;\"><a href=\"#tf-idf-%ED%99%9C%EC%9A%A9\" aria-label=\"tf idf 활용 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TF-IDF 활용</h3>\n<p>document에서 너무 자주 등장하는 단어의 경우, 별로 유의미하지 않음에도 <strong>TF</strong>(Term Frequency) 가중치가 높아집니다. 이로 인해 정말로 중요한 단어들이 가려지게 됩니다.<br>\n<br>\n이를 방지하기 위해  <strong>IDF</strong>(Inverse document frequency)를 사용합니다.<br>\n<strong>IDF</strong>는 해당 단어가 등장하는 documents의 개수에 inverse function을 취합니다.<br>\n따라서 자주 등장하는 단어일수록 가중치를 낮추는 효과를 보입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\">vectorizer <span class=\"token operator\">=</span> TfidfVectorizer<span class=\"token punctuation\">(</span>use_idf<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\r\nX <span class=\"token operator\">=</span> vectorizer<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>corpus<span class=\"token punctuation\">)</span></code></pre></div>\n<p>위의 코드에서 <code class=\"language-text\">use_idf</code>를 통해 IDF기능을 사용합니다.</p>\n<h3 id=\"normalized-term-frequencies\" style=\"position:relative;\"><a href=\"#normalized-term-frequencies\" aria-label=\"normalized term frequencies permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Normalized Term Frequencies</h3>\n<p>특정 document가 엄청 길기 때문에 단어가 많이 카운트된다면, 가중치 선정이 불공평해집니다.<br>\n따라서 <strong>Normalize</strong> 는 document의 길이를 고려하여 단순히 긴 문서의 편향을 낮춥니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\">vectorizer <span class=\"token operator\">=</span> TfidfVectorizer<span class=\"token punctuation\">(</span>norm<span class=\"token operator\">=</span><span class=\"token string\">\"l2\"</span><span class=\"token punctuation\">)</span>\r\nX <span class=\"token operator\">=</span> vectorizer<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>corpus<span class=\"token punctuation\">)</span></code></pre></div>\n<p>위의 코드에서 <code class=\"language-text\">norm</code>를 통해 <strong>Normalize</strong> 기능을 사용합니다. 옵션은 다음과 같습니다.</p>\n<ul>\n<li>l2: Sum of squares of vector elements is 1</li>\n<li>l1: Sum of absolute values of vector elements is 1</li>\n</ul>\n<h3 id=\"corpus-specific-stopwords\" style=\"position:relative;\"><a href=\"#corpus-specific-stopwords\" aria-label=\"corpus specific stopwords permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Corpus-Specific Stopwords</h3>\n<p>IDF와 비슷한 맥락으로, 문서 전체에서 특정 비율 이상으로 나타나는 단어를 Stopwords로 제외하는 기능입니다.</p>\n<blockquote>\n<p><code class=\"language-text\">max_df</code> (float in range [0.0, 1.0] or int, default=1.0):<br>\nWhen building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold. For example, if max_df is set to 0.9, all terms that appear in over 90% of the documents will be excluded.</p>\n</blockquote>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\">vectorizer <span class=\"token operator\">=</span> TfidfVectorizer<span class=\"token punctuation\">(</span>max_df<span class=\"token operator\">=</span><span class=\"token number\">0.9</span><span class=\"token punctuation\">)</span>\r\nX <span class=\"token operator\">=</span> vectorizer<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>corpus<span class=\"token punctuation\">)</span></code></pre></div>\n<p>위의 코드에서 <code class=\"language-text\">max_df=0.9</code> 부분에 해당합니다.</p>\n<h3 id=\"k-means\" style=\"position:relative;\"><a href=\"#k-means\" aria-label=\"k means permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>K-means</h3>\n<p>다음은 총 5개의 클러스터로 데이터 프레임의 body를 분류하는 코드입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>feature_extraction<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> TfidfVectorizer\r\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>cluster <span class=\"token keyword\">import</span> KMeans\r\n\r\nvectorizer <span class=\"token operator\">=</span> TfidfVectorizer<span class=\"token punctuation\">(</span>use_idf<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> norm<span class=\"token operator\">=</span><span class=\"token string\">\"l2\"</span><span class=\"token punctuation\">,</span> stop_words<span class=\"token operator\">=</span><span class=\"token string\">\"english\"</span><span class=\"token punctuation\">,</span> max_df<span class=\"token operator\">=</span><span class=\"token number\">0.7</span><span class=\"token punctuation\">)</span>\r\nX <span class=\"token operator\">=</span> vectorizer<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>df<span class=\"token punctuation\">.</span>body<span class=\"token punctuation\">)</span>\r\n\r\nk <span class=\"token operator\">=</span> <span class=\"token number\">5</span>\r\n\r\nkmeans <span class=\"token operator\">=</span> KMeans<span class=\"token punctuation\">(</span>n_clusters<span class=\"token operator\">=</span>k<span class=\"token punctuation\">)</span>\r\n<span class=\"token operator\">%</span>time kmeans<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span>\r\n\r\ndf<span class=\"token punctuation\">[</span><span class=\"token string\">\"cluster\"</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> kmeans<span class=\"token punctuation\">.</span>labels_\r\ndf<span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"text\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"cluster\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p><code class=\"language-text\">df[\"cluster\"] = kmeans.labels_</code> 부분은 데이터 프레임에 <code class=\"language-text\">cluster</code>라는 새로운 colum을 만들어 주고 해당 raw 데이터의 cluster 번호를 채워줍니다.<br>\n<br>\n아래의 코드를 실행시켜 보면 각 클러스터가 얼마만큼씩 있는지 알려줍니다.<br>\n당연히 분포가 고르면 고를수록 균형이 잘 잡혀있는 좋은 결과입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\">df<span class=\"token punctuation\">.</span>cluster<span class=\"token punctuation\">.</span>value_counts<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h3 id=\"lda-topic-modeling\" style=\"position:relative;\"><a href=\"#lda-topic-modeling\" aria-label=\"lda topic modeling permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>LDA Topic Modeling</h3>\n<p>총 5개의 토픽으로 데이터 프레임의 body를 분류하는 코드입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>decomposition <span class=\"token keyword\">import</span> LatentDirichletAllocation <span class=\"token keyword\">as</span> LDA\r\n\r\nnum_topics <span class=\"token operator\">=</span> <span class=\"token number\">5</span>\r\n\r\nlda <span class=\"token operator\">=</span> LDA<span class=\"token punctuation\">(</span>n_components<span class=\"token operator\">=</span>num_topics<span class=\"token punctuation\">)</span>\r\n<span class=\"token operator\">%</span>time lda<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span></code></pre></div>\n<p><br>\nLDA의 결과를 보고 싶다면 다음과 같은 함수를 실행시켜 주면 됩니다.<br>\n해당 함수는 토픽 내 각 단어들의 가중치들을 보여줍니다. 가중치가 높을수록 해당 토픽과 깊은 연관이 있다는 뜻이며, 잘 분류되었다고 볼 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">def</span> <span class=\"token function\">show_topics</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> feature_names<span class=\"token punctuation\">,</span> num_top_words<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\r\n    <span class=\"token keyword\">for</span> topic_idx<span class=\"token punctuation\">,</span> topic_scores <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>components_<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\r\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"***Topic {}:\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>topic_idx<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\r\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\" + \"</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"{:.2f} * {}\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>topic_scores<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> feature_names<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> topic_scores<span class=\"token punctuation\">.</span>argsort<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span>num_top_words<span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\r\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><br>\n만약 시각화를 하고 싶다면, 다음과 같이 <code class=\"language-text\">pyLDAvis</code>를 사용해 주면 됩니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">import</span> pyLDAvis\r\n<span class=\"token keyword\">import</span> pyLDAvis<span class=\"token punctuation\">.</span>sklearn\r\npyLDAvis<span class=\"token punctuation\">.</span>enable_notebook<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\r\n\r\npyLDAvis<span class=\"token punctuation\">.</span>sklearn<span class=\"token punctuation\">.</span>prepare<span class=\"token punctuation\">(</span>lda<span class=\"token punctuation\">,</span> X<span class=\"token punctuation\">,</span> vectorizer<span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 768px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/030460581402d2a327c9771fca9fa7da/1d227/4.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.5%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACAUlEQVQ4y31Ti26bMBTl/79mW6dF1TRpUjdpk9Yur2aqkpCG8GiCg02wAWObM9kkKYm6WTr4+sHhnnsPnjEGnHMIIdxsURQFhOj27LkdWhvUSkNqA17VIDR3SLMDqloe72h4yrT432iPz1W8w2Ca4PYxwefZC74tCO7mBLPkAH3maOFNQgoqarc0bYv2CnbPXlyGW3wYRRhMYkwjBl4rKG2gjYExrSO1a++nTxDsRcd/JDnF52EUntYx3g9jfPnzgh8+wTCgmEQM47DDaEOx2BXwNpmAVPqC8MRVNxppUYMeOJ6eI3wcJ7ib71wCCasQ0RIx6xDSElRIeDhKOkt0TWhRSYVpSPE7oLj3t/g+9XEzjmEVrQjHMi3gpxyrlGO+LZCJrjFeX15H2MUJK/EQZJ2cNcHg1xzvhhHu15nLxirro2o6ld6WXdbPHMnTonIvj0KGh1WK24cFPk0T9wGbWZ8syAQa3dnL43Vztke/KTbTNeEYb6iTNAt2uBmGGIcUpKjBygZMSFe3unntgadVAymbK7u8ulAqA6MaLGOCr35HviICz3vh5k1WnuW6DIUoUVWVc/l1PfvWyUmGxeMSjAuIQ45KcChZA625uO/ZriqlIKV804unWAmBZLHEjmTY7/eglIEyBkopcpYjz5n7Vb2+1H9l6OI36nyymVVnYTP8C9E+53yUMIKuAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/030460581402d2a327c9771fca9fa7da/a59e9/4.webp 192w,\n/static/030460581402d2a327c9771fca9fa7da/0ca9f/4.webp 384w,\n/static/030460581402d2a327c9771fca9fa7da/dc9b9/4.webp 768w,\n/static/030460581402d2a327c9771fca9fa7da/e2c2f/4.webp 1152w,\n/static/030460581402d2a327c9771fca9fa7da/ae3d9/4.webp 1164w\"\n              sizes=\"(max-width: 768px) 100vw, 768px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/030460581402d2a327c9771fca9fa7da/3b721/4.png 192w,\n/static/030460581402d2a327c9771fca9fa7da/66595/4.png 384w,\n/static/030460581402d2a327c9771fca9fa7da/fe486/4.png 768w,\n/static/030460581402d2a327c9771fca9fa7da/d2d74/4.png 1152w,\n/static/030460581402d2a327c9771fca9fa7da/1d227/4.png 1164w\"\n            sizes=\"(max-width: 768px) 100vw, 768px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/030460581402d2a327c9771fca9fa7da/fe486/4.png\"\n            alt=\"4\"\n            title=\"4\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<h2 id=\"3-느낀-점\" style=\"position:relative;\"><a href=\"#3-%EB%8A%90%EB%82%80-%EC%A0%90\" aria-label=\"3 느낀 점 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. 느낀 점</h2>\n<p>k-Means Clustering, Topic Modeling 분석을 써보면서 세상에는 정말로 똑똑한 사람들이 많다는 생각을 했습니다. 이렇게 강력한 분석을 간단하게 이용할 수 있도록 미리 다 만들어 두었다는 점이 대단했기 때문입니다. 😅<br>\n이번에는 간단한 원리와 과정만 알아보긴 했지만, 그래도 여러 가지를 많이 배운 느낌입니다. 나중에 좀 더 수학적, 공학적으로 깊은 이해를 위해 노력해야겠습니다. 😭</p>\n<h2 id=\"source\" style=\"position:relative;\"><a href=\"#source\" aria-label=\"source permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Source</h2>\n<ul>\n<li>성균관대학교 SW Coaching 프로그램</li>\n<li>이강표(Kang-Pyo Lee) 박사님 강의</li>\n</ul>\n<!--\r\n1주 Web Scraping 기초\r\n2주 Web Scraping 심화\r\n3주 Pandas Dataframe 다루기\r\n4주 텍스트 데이터 처리\r\n5주 문서 클러스터링 및 토픽 모델링\r\n6주 개인 프로젝트 발표\r\n-->","id":"bac3b428-f0db-53a4-940a-d685b83440c2","tableOfContents":"<ul>\n<li>\n<p><a href=\"#1-%EA%B0%95%EC%9D%98-%EC%A0%95%EB%A6%AC\">1. 강의 정리</a></p>\n<ul>\n<li><a href=\"#k-means-clustering\">k-Means Clustering</a></li>\n<li><a href=\"#topic-modeling\">Topic Modeling</a></li>\n<li><a href=\"#tip\">⭐TIP!⭐</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#2-%EC%BD%94%EB%93%9C-%EC%BD%94%EC%B9%AD\">2. 코드 코칭</a></p>\n<ul>\n<li><a href=\"#tf-idf-%ED%99%9C%EC%9A%A9\">TF-IDF 활용</a></li>\n<li><a href=\"#normalized-term-frequencies\">Normalized Term Frequencies</a></li>\n<li><a href=\"#corpus-specific-stopwords\">Corpus-Specific Stopwords</a></li>\n<li><a href=\"#k-means\">K-means</a></li>\n<li><a href=\"#lda-topic-modeling\">LDA Topic Modeling</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#3-%EB%8A%90%EB%82%80-%EC%A0%90\">3. 느낀 점</a></p>\n</li>\n<li>\n<p><a href=\"#source\">Source</a></p>\n</li>\n</ul>","frontmatter":{"title":"[SW Coaching - Week 5] 문서 클러스터링 및 토픽 모델링","summary":"파이썬 Web Scraping, TF-IDF, Normalized TF, K-means, LDA, pyLDAvis","date":"2023.02.10.","categories":["Python","WebScraping","SW Coaching"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABBUlEQVQoz4VTywqEMAzs1/h+ov6BioJvBAXx4MGz/3/KMoG6XdH1MEyahqGTpMJxHHJdl+F53g/U3L9YAlpCXoJ1XWcYhsEwTfOM1bw8A6hRNYRUj6KIqqqiYRioLEsqioLSNOUY+bquOQfO85yapqG+7ynLMgqC4HylkMoQnKaJlmVhjOPIQojneT7vwBAGr+tKXddRGIZfwTfLtm0zLMtiSJtgTdO47taybLLv+yfQ5G3b6DgO2vedX4O8WncdjnianjzDTpIk3BL0Sq27qxd366IWwCYsgmH9bYXE3U6phW3b8oAwXUxc3bvrHv4VVC3HccysWn76CK+C8gfIH3UVuL72A+RPXpKwJ9vxAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/1f00240a23b50e7f7e4cda9c0da81aba/7d213/title.png","srcSet":"/static/1f00240a23b50e7f7e4cda9c0da81aba/504b1/title.png 199w,\n/static/1f00240a23b50e7f7e4cda9c0da81aba/3a37f/title.png 398w,\n/static/1f00240a23b50e7f7e4cda9c0da81aba/7d213/title.png 796w","sizes":"(min-width: 796px) 796px, 100vw"},"sources":[{"srcSet":"/static/1f00240a23b50e7f7e4cda9c0da81aba/af7f4/title.webp 199w,\n/static/1f00240a23b50e7f7e4cda9c0da81aba/98ae6/title.webp 398w,\n/static/1f00240a23b50e7f7e4cda9c0da81aba/f27f1/title.webp 796w","type":"image/webp","sizes":"(min-width: 796px) 796px, 100vw"}]},"width":796,"height":448}},"publicURL":"/static/1f00240a23b50e7f7e4cda9c0da81aba/title.PNG"}}}}]}},"pageContext":{"slug":"/sw-coaching-week-5/"}},"staticQueryHashes":[]}