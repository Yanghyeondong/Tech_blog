---
date: '2023-01-09'
title: '[SW Coaching - Week 2] Web Scraping ì‹¬í™”'
categories: ['Python','Web Scraping','SW Coaching']
summary: 'íŒŒì´ì¬ Web Scraping, html ë° csvë¡œ ì €ì¥, time.sleep() ìœ„ì¹˜'
thumbnail: './sw-coaching-week-2/title.png'
---

*ë³¸ í¬ìŠ¤íŠ¸ëŠ” IOWA ëŒ€í•™ **ì´ê°•í‘œ**(Kang-Pyo Lee) ë°•ì‚¬ë‹˜ì˜ í—ˆë½ì„ êµ¬í•˜ê³  ê°•ì˜ë¥¼ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤.*  
*ê°•ì˜ ì‚¬ì§„, ì½”ë“œì˜ ì €ì‘ê¶Œì€ ëª¨ë‘ ì´ê°•í‘œ ë°•ì‚¬ë‹˜ê»˜ ìˆìŠµë‹ˆë‹¤.*


## 1. ê°•ì˜ ì •ë¦¬
ì´ë²ˆ ê°•ì˜ëŠ” ì´ë¡ ë³´ë‹¤ëŠ” ì‹¤ìŠµ ìœ„ì£¼ë¡œ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ì¼ë¶€ ì¤‘ìš”ì ë§Œ ì •ë¦¬í•˜ì—¬ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.   

### urlsë¡œ html ê°€ì ¸ì˜¤ê¸°
ì›¹í˜ì´ì§€ì—ì„œ **features** ê°™ì´ ì—¬ëŸ¬ í˜ì´ì§€ê°€ ëª©ë¡í™”ë˜ì–´ìˆëŠ” í˜ì´ì§€ë¥¼ ì°¾ìŠµë‹ˆë‹¤.  
í•´ë‹¹ í˜ì´ì§€ì—ì„œ ê° **url**ì„ ì¶”ì¶œí•œ ë‹¤ìŒ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **html** íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ê³  ì €ì¥í•©ë‹ˆë‹¤.  
```py
urls = ["https://fivethirtyeight.com/features/"]

# The range(2, 11) generates a list of integers from 2 to 10.
for i in range(2, 11):
    url = f"https://fivethirtyeight.com/features/page/{i}/"
    urls.append(url)

for url in urls:
    print(url)

    # Get the content of a pag
    r = requests.get(url)
    soup = BeautifulSoup(r.content, "html.parser")

    # Get the list of article
    h2_list = soup.find_all("h2", {"class": "article-title entry-title"})
    
    for h2 in h2_list:
        # Find the anchor tag
        a = h2.find("a")
        
        # Extract the title & URL of an article
        title = a.text
        article_url = a["href"]
        
        # Fetch the content and save it as an HTML file
        print("- " + article_url + ": processing...")
        
        r2 = requests.get(article_url)
                
        if "/features/" in article_url:
          file_name = article_url[len("https://fivethirtyeight.com/features/"):-1] + ".html"
        elif "/videos/" in article_url:
          file_name = article_url[len("https://fivethirtyeight.com/videos/"):-1] + ".html"
        elif "/methodology/" in article_url:
          file_name = article_url[len("https://fivethirtyeight.com/methodology/"):-1] + ".html"
        elif "/live-blog/" in article_url:
          file_name = article_url[len("https://fivethirtyeight.com/live-blog/"):-1] + ".html"
        else:
          assert 0 == 1, "Unknown article url pattern!"

        with open(f"{outcome_folder}/HTMLs/{file_name}", "w+b") as fw:
            fw.write(r2.content)
        
        print("- " + file_name + ": saved.")
        
        # Sleep for a second to not overload the web site
        time.sleep(1)
```
html íŒŒì¼ì„ **ì €ì¥**í•˜ëŠ” ì´ìœ   
= ì›¹í˜ì´ì§€ íŒŒì¼ì€ ì–¸ì œë“ ì§€ ë°”ë€” ìˆ˜ ìˆê¸° ë•Œë¬¸ì—  
\
`article_url[len("https://fivethirtyeight.com/live-blog/"):-1]`  
= ì›¹ ì‚¬ì´íŠ¸ì—ì„œ í•´ë‹¹ í¬ìŠ¤íŠ¸ì˜ ì´ë¦„ë§Œ ê°€ì ¸ì˜¤ê¸° ìœ„í•´  
= url íŒ¨í„´ì„ ë¶„ë¥˜í•˜ê¸° ìœ„í•´  
\
`time.sleep(1)`  
= **politeness policy**ë¥¼ ì§€í‚¤ê¸° ìœ„í•´

### csv íŒŒì¼ë¡œ ì €ì¥  

ë‹¤ìš´ë°›ì€ **html** íŒŒì¼ì—ì„œ í•„ìš”í•œ ìë£Œ **file_name, article_title, article_author** ë§Œ ì¶”ì¶œí•˜ì—¬ **csv** íŒŒì¼ì„ ë§Œë“­ë‹ˆë‹¤.  
```py
with open(f"{outcome_folder}/html_metadata.csv", "w") as fw:
    # Column names on the first row
    fw.write("file_name\tarticle_title\tarticle_author\n")

    for file_name in os.listdir(f"{outcome_folder}/HTMLs"):
        if not file_name.endswith(".html"):
            continue
        
        # Column values starting from the second row
        with open(f"{outcome_folder}/HTMLs/{file_name}", "r+b") as fr:
            print(file_name)
            soup = BeautifulSoup(fr.read(), "html.parser")
            
            # No title exception handling
            if soup.find("h1", {"class": "article-title article-title-single entry-title"}) == None:
                article_title = ""
            else:
                article_title = soup.find("h1", {"class": "article-title article-title-single entry-title"}).text.strip()
            
            # No author exception handling
            if soup.find("a", {"class": "author url fn"}) == None:
                article_author = ""
            else:
                article_author = soup.find("a", {"class": "author url fn"}).text
            
            # Remove all possible tabs
            article_title = article_title.replace("\t", "")
            article_aurthor = article_author.replace("\t", "")
                        
            fw.write(f"{file_name}\t{article_title}\t{article_author}\n")
```

`\t`ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ   
= article_titleì— `,`ê°€ ìˆì„ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ê¸°ë³¸ êµ¬ë¶„ìë¥¼ `\t`ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.  
\
`if soup.find("a", {"class": "author url fn"}) == None:`  
= article_authorê°€ ì—†ëŠ” ê²½ìš°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.  article_titleë„ ë™ì¼í•©ë‹ˆë‹¤.  

## 2. ì½”ë“œ ì½”ì¹­
*ì½”ë“œ ì „ë¬¸ì€ ë¶„ëŸ‰ìƒ ì œì™¸í•˜ì˜€ìœ¼ë©°, í”¼ë“œë°± ìœ„ì£¼ë¡œ ì •ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤. Colab í™˜ê²½ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.*  
### hierarchy ì‚¬ìš© ê¸°ì¤€
ì•„ë˜ì˜ ì›¹ì‚¬ì´íŠ¸ì—ì„œ í¬ìŠ¤íŠ¸ì˜ excerpt, authorë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì½”ë“œë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
[https://www.kdnuggets.com/2021/05/top-programming-languages.html](https://www.kdnuggets.com/2021/05/top-programming-languages.html)  
**ì½”ë“œ 1**
```py
ans2_1 = soup.find("div", attrs={"class": "post-header-has-award"}).find("p", attrs={"class": "excerpt"}).text.strip()
ans2_2 = soup.find("div", attrs={"class": "author-link"}).find("a", attrs={"rel": "author"}).text.strip()
```
**ì½”ë“œ 2**
```py
ans2_1 = soup.find("p", attrs={"class": "excerpt"}).text.strip()
ans2_2 = soup.find("a", attrs={"rel": "author"}).text.strip()
```
**ì½”ë“œ 1**ì˜ ê²½ìš° hierarchyë¥¼ ì˜ì‹í•´ì„œ ë¶€ëª¨ì˜ classë¥¼ ì§€ì •í•´ ì£¼ì—ˆì§€ë§Œ, ì°¾ê³ ì í•˜ëŠ” ëŒ€ìƒì˜ elementë‚˜ attrsê°€ ì •í™•í•˜ë‹¤ë©´ **êµ³ì´ hierarchyë¥¼ ê³ ì§‘í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤**. ë”°ë¼ì„œ **ì½”ë“œ 2**ê°€ ë” ë°”ëŒì§í•˜ë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### í•„ìš”í•œ ì‚¬ì§„ ê²½ë¡œë§Œ ë¶ˆëŸ¬ì˜¤ê¸°
ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë³¸ë¬¸ì˜ ì‚¬ì§„ì„ ë¶ˆëŸ¬ì˜¤ë©´ ì•„ë˜ì²˜ëŸ¼ ë‚˜ì˜µë‹ˆë‹¤.  
```py
imgs = soup.find("div", attrs={"id": "post-"}).find_all("img")

for img in imgs:
    print(img["src"])
```
```py
data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201000%200'%3E%3C/svg%3E
https://www.kdnuggets.com/wp-content/uploads/Costa_top-programming-languages.jpg
data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20271%20262'%3E%3C/svg%3E
https://www.kdnuggets.com/wp-content/uploads/Costa_Python_top-programming-languages.jpg
data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20263%20297'%3E%3C/svg%3E
https://www.kdnuggets.com/wp-content/uploads/Costa_JavaScript_top-programming-languages.jpg
data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20199%20264'%3E%3C/svg%3E
https://www.kdnuggets.com/wp-content/uploads/Costa_Java_top-programming-languages.jpg
data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20350%20271'%3E%3C/svg%3E
https://www.kdnuggets.com/wp-content/uploads/Costa_R_top-programming-languages.jpg
data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20350%20168'%3E%3C/svg%3E
https://www.kdnuggets.com/wp-content/uploads/Costa_C_top-programming-languages.jpg
data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20216%20297'%3E%3C/svg%3E
https://www.kdnuggets.com/wp-content/uploads/Costa_Go_top-programming-languages.jpg
data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20284%20313'%3E%3C/svg%3E
https://www.kdnuggets.com/wp-content/uploads/Costa_Csharp_top-programming-languages.jpg
data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20350%20196'%3E%3C/svg%3E
https://www.kdnuggets.com/wp-content/uploads/Costa_PHP_top-programming-languages.jpg
data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20313%20313'%3E%3C/svg%3E
https://www.kdnuggets.com/wp-content/uploads/Costa_Swift_top-programming-languages.jpg
```
ì´ ì¤‘ì—ì„œ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ `.endswith()`ìœ¼ë¡œ ë§¤ìš° ê°„ë‹¨íˆ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
```py
for img in img_list: 
    if not img["src"].endswith(".jpg"): 
        continue 
    print(img["src"]) 
```
```py
https://www.kdnuggets.com/wp-content/uploads/Costa_top-programming-languages.jpg
https://www.kdnuggets.com/wp-content/uploads/Costa_Python_top-programming-languages.jpg
https://www.kdnuggets.com/wp-content/uploads/Costa_JavaScript_top-programming-languages.jpg
https://www.kdnuggets.com/wp-content/uploads/Costa_Java_top-programming-languages.jpg
https://www.kdnuggets.com/wp-content/uploads/Costa_R_top-programming-languages.jpg
https://www.kdnuggets.com/wp-content/uploads/Costa_C_top-programming-languages.jpg
https://www.kdnuggets.com/wp-content/uploads/Costa_Go_top-programming-languages.jpg
https://www.kdnuggets.com/wp-content/uploads/Costa_Csharp_top-programming-languages.jpg
https://www.kdnuggets.com/wp-content/uploads/Costa_PHP_top-programming-languages.jpg
https://www.kdnuggets.com/wp-content/uploads/Costa_Swift_top-programming-languages.jpg
```

### time.sleep()ì˜ ìœ„ì¹˜
ë‹¤ìŒ WHO ì›¹í˜ì´ì§€ ëª©ë¡ì—ì„œ í•„ìš”í•œ ì •ë³´(ì œëª©, url)ë¥¼ ì¶”ì¶œí•´ì„œ csvë¡œ ì €ì¥í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.  
[https://www.who.int/news-room/releases/1/](https://www.who.int/news-room/releases/1/)
```py
import time
import os

urls = []

for i in range(1, 6):
    url = f"https://www.who.int/news-room/releases/{i}/"
    urls.append(url)

with open(f"{outcome_folder}/hw2_output.csv", "w") as fw:
    fw.write("title\turl\n")

    for url in urls:
        r = requests.get(url)
        soup3 = BeautifulSoup(r.content, "html.parser")
        a_list = soup3.find("div", attrs={"class": "list-view vertical-list vertical-list--image"}).find_all("a", attrs={"class": "link-container table"})
        
        for a in a_list:
            
            title = a.find("p", attrs={"class": "heading text-underline"}).text.replace("\n", " ").replace("\r", " ").strip()
            url = "https://www.who.int"+a["href"]
            fw.write(f"{title}\t{url}\n")

        time.sleep(1)
```

ì—¬ê¸°ì„œ `time.sleep()`ì˜ ìœ„ì¹˜ê°€ ì¤‘ì²© ë°˜ë³µë¬¸ì˜ ì–´ëŠ ìœ„ì¹˜ì— ìˆëŠ”ì§€ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤. ë‚´ë¶€ ë°˜ë³µë¬¸ì€ í•œ í˜ì´ì§€ ë‚´ì—ì„œ elementë¥¼ ì°¾ëŠ” ê²ƒì´ë¯€ë¡œ `time.sleep()`ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ìƒˆë¡œ í˜ì´ì§€ë¥¼ request í•˜ëŠ” ë°”ê¹¥ ë°˜ë³µë¬¸ì— ìœ„ì¹˜í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤.  
- `time.sleep()` ì˜ ìœ„ì¹˜ == `requests.get(url)` ì˜ ìœ„ì¹˜  

## 3. ëŠë‚€ ì 
ì €ë²ˆì£¼ì— ì´ì–´ **Web scraping**ì— ëŒ€í•´ ë§ì€ ê²ƒì„ ë°°ìš´ ì¢‹ì€ ì‹œê°„ì´ì—ˆìŠµë‹ˆë‹¤. ë‹¨ìˆœíˆ ë°°ìš°ê³  ê³¼ì œë¥¼ í•˜ëŠ” ê²ƒì— ê·¸ì¹˜ì§€ ì•Šê³ , ê°œì¸ í”„ë¡œì íŠ¸ì˜ ìœ¤ê³½ì„ ì¡ìœ¼ë©´ì„œ ìŠ¤ìŠ¤ë¡œ ê³µë¶€ë¥¼ ë” í•´ë‚˜ê°€ë„ë¡ ë…¸ë ¥í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤ ğŸ˜Š.


## Source
- ì„±ê· ê´€ëŒ€í•™êµ SW Coaching í”„ë¡œê·¸ë¨  
- ì´ê°•í‘œ(Kang-Pyo Lee) ë°•ì‚¬ë‹˜ ê°•ì˜  

<!--
1ì£¼ Web Scraping ê¸°ì´ˆ
2ì£¼ Web Scraping ì‹¬í™”
3ì£¼ Pandas Dataframe ë‹¤ë£¨ê¸°
4ì£¼ í…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬
5ì£¼ ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ë° í† í”½ ëª¨ë¸ë§
6ì£¼ ê°œì¸ í”„ë¡œì íŠ¸ ë°œí‘œ
-->