---
date: '2023-02-10'
title: '[SW Coaching - Week 5] ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ë° í† í”½ ëª¨ë¸ë§'
categories: ['Python','Web Scraping','SW Coaching']
summary: 'íŒŒì´ì¬ Web Scraping, TF-IDF, Normalized TF, K-means, LDA, pyLDAvis'
thumbnail: './sw-coaching-week-5/title.PNG'
---

*ë³¸ í¬ìŠ¤íŠ¸ëŠ” IOWA ëŒ€í•™ **ì´ê°•í‘œ**(Kang-Pyo Lee) ë°•ì‚¬ë‹˜ì˜ í—ˆë½ì„ êµ¬í•˜ê³  ê°•ì˜ë¥¼ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤.*  
*ê°•ì˜ ì‚¬ì§„, ì½”ë“œì˜ ì €ì‘ê¶Œì€ ëª¨ë‘ ì´ê°•í‘œ ë°•ì‚¬ë‹˜ê»˜ ìˆìŠµë‹ˆë‹¤.*


## 1. ê°•ì˜ ì •ë¦¬

### k-Means Clustering  

**íŠ¹ì§•:**  
- simplest, most commonly used  
- ë°ì´í„°ì—ì„œ íŠ¹ì • ì˜ì—­ì„ ëŒ€í‘œí•˜ëŠ” centroids ë¥¼ í™œìš©.  
\
**ê³¼ì •:** 
1. Kê°œì˜ clusterì— ëŒ€í•œ Kê°œì˜ centroids ë“¤ì„ ëœë¤ í•˜ê²Œ ì„¤ì •  
2. ë‹¤ìŒì˜ 3,4ë²ˆì„ ë°˜ë³µ
3. ê° ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•´ centroids ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ì—¬ ê°€ì¥ ê°€ê¹Œìš´ centroidì— í• ë‹¹.
4. ê°™ì´ í• ë‹¹ëœ ë°ì´í„° í¬ì¸íŠ¸ë“¤ ì‚¬ì´ì—ì„œ í‰ê· ì„ êµ¬í•´ centroid ë¥¼ ì—…ë°ì´íŠ¸.
5. ë” ì´ìƒ centroids ê°€ ë³€í•˜ì§€ ì•Šì„ ê²½ìš° ì¢…ë£Œ.  
\
![1](./sw-coaching-week-5/1.png)  
\
**ì¥ì :**  
- ìƒëŒ€ì ìœ¼ë¡œ ì´í•´í•˜ê³  ì ìš©í•˜ê¸° ì‰¬ì›€.  
- ì²˜ë¦¬ ì‹œê°„ì´ ì§§ìŒ.  
- í° ë°ì´í„°ì—ë„ ì‚¬ìš©ì´ ìš©ì´.  
**ë‹¨ì :**  
- random initialization ì— ì˜ì¡´í•˜ê²Œ ëœë‹¤.  
- ê°„ë‹¨í•œ í˜•íƒœì˜ cluster ë§Œ ìº¡ì³ê°€ ê°€ëŠ¥í•˜ë‹¤.  
- cluster ì— ëŒ€í•´ all directions ì˜ important ë˜‘ê°™ë‹¤. ì¦‰, ê°€ì¤‘ì¹˜ê°€ ë™ì¼í•˜ë‹¤.  
- ì ì ˆí•œ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ k ë¥¼ ì‚¬ìš©ìê°€ ì§ì ‘ ì •í•´ì¤˜ì•¼ í•œë‹¤.  

### Topic Modeling  

**ê°œë…:**  
- abstract(ì¶”ìƒì ), latent(ìˆ¨ê²¨ì§„) í† í”½ì„ ì°¾ì•„ë‚´ì„œ model ì œì‘. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ document ë¶„ë¥˜.    
- Latent Dirichlet Allocation (LDA)ê°€ ì¢‹ì€ ì˜ˆì‹œ.  
\
**Latent Dirichlet Allocation (LDA)í•µì‹¬**  
- documentëŠ” íŠ¹ì • í† í”½ ë‚´ì˜ ë‹¨ì–´ë“¤ì„ ì¡°í•©í•˜ì—¬ ë§Œë“¤ì–´ì§„ë‹¤ê³  ê°€ì •.  
- topics ë¥¼ ì°¾ê¸° ìœ„í•´ document, words ë¥¼ reverse engineers í•œë‹¤.  

![2](./sw-coaching-week-5/2.png)  

- document, words matrix ì— topics ë¼ëŠ” ìƒˆë¡œìš´ ì¶•ì„ ì¶”ê°€í•´ decompose í•œë‹¤.  
  = singular valuede composition (SVD)



### â­TIP!â­

Topic Modeling ê³¼ k-Means Clustering ëª¨ë‘ **Unsupervised Learning** ì„!  
\
ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ë° í† í”½ ëª¨ë¸ë§ ë¹„êµ
![3](./sw-coaching-week-5/3.png) 

## 2. ì½”ë“œ ì½”ì¹­
*ì½”ë“œ ì „ë¬¸ì€ ë¶„ëŸ‰ìƒ ì œì™¸í•˜ì˜€ìœ¼ë©°, í”¼ë“œë°± ìœ„ì£¼ë¡œ ì •ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤. Colab í™˜ê²½ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.*  
\
ì´ë²ˆ ì½”ë“œ ì½”ì¹­ì€ í”¼ë“œë°±ë³´ë‹¤ëŠ” **ì„¤ëª… ìœ„ì£¼**ë¡œ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.  

### TF-IDF í™œìš©  

documentì—ì„œ ë„ˆë¬´ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì˜ ê²½ìš°, ë³„ë¡œ ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒì—ë„ **TF**(Term Frequency) ê°€ì¤‘ì¹˜ê°€ ë†’ì•„ì§‘ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ì •ë§ë¡œ ì¤‘ìš”í•œ ë‹¨ì–´ë“¤ì´ ê°€ë ¤ì§€ê²Œ ë©ë‹ˆë‹¤.  
\
ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´  **IDF**(Inverse document frequency)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.  
**IDF**ëŠ” í•´ë‹¹ ë‹¨ì–´ê°€ ë“±ì¥í•˜ëŠ” documentsì˜ ê°œìˆ˜ì— inverse functionì„ ì·¨í•©ë‹ˆë‹¤.  
ë”°ë¼ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì¼ìˆ˜ë¡ ê°€ì¤‘ì¹˜ë¥¼ ë‚®ì¶”ëŠ” íš¨ê³¼ë¥¼ ë³´ì…ë‹ˆë‹¤.  
```py
vectorizer = TfidfVectorizer(use_idf=True)
X = vectorizer.fit_transform(corpus)
```
ìœ„ì˜ ì½”ë“œì—ì„œ `use_idf`ë¥¼ í†µí•´ IDFê¸°ëŠ¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

### Normalized Term Frequencies

íŠ¹ì • documentê°€ ì—„ì²­ ê¸¸ê¸° ë•Œë¬¸ì— ë‹¨ì–´ê°€ ë§ì´ ì¹´ìš´íŠ¸ëœë‹¤ë©´, ê°€ì¤‘ì¹˜ ì„ ì •ì´ ë¶ˆê³µí‰í•´ì§‘ë‹ˆë‹¤.  
ë”°ë¼ì„œ **Normalize** ëŠ” documentì˜ ê¸¸ì´ë¥¼ ê³ ë ¤í•˜ì—¬ ë‹¨ìˆœíˆ ê¸´ ë¬¸ì„œì˜ í¸í–¥ì„ ë‚®ì¶¥ë‹ˆë‹¤.  
```py
vectorizer = TfidfVectorizer(norm="l2")
X = vectorizer.fit_transform(corpus)
```
ìœ„ì˜ ì½”ë“œì—ì„œ `norm`ë¥¼ í†µí•´ **Normalize** ê¸°ëŠ¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì˜µì…˜ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.  
- l2: Sum of squares of vector elements is 1
- l1: Sum of absolute values of vector elements is 1

### Corpus-Specific Stopwords

IDFì™€ ë¹„ìŠ·í•œ ë§¥ë½ìœ¼ë¡œ, ë¬¸ì„œ ì „ì²´ì—ì„œ íŠ¹ì • ë¹„ìœ¨ ì´ìƒìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ë¥¼ Stopwordsë¡œ ì œì™¸í•˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤.  
> `max_df` (float in range [0.0, 1.0] or int, default=1.0):  
> When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold. For example, if max_df is set to 0.9, all terms that appear in over 90% of the documents will be excluded.
```py
vectorizer = TfidfVectorizer(max_df=0.9)
X = vectorizer.fit_transform(corpus)
```
ìœ„ì˜ ì½”ë“œì—ì„œ `max_df=0.9` ë¶€ë¶„ì— í•´ë‹¹í•©ë‹ˆë‹¤.  

### K-means
ë‹¤ìŒì€ ì´ 5ê°œì˜ í´ëŸ¬ìŠ¤í„°ë¡œ ë°ì´í„° í”„ë ˆì„ì˜ bodyë¥¼ ë¶„ë¥˜í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.  

```py
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

vectorizer = TfidfVectorizer(use_idf=True, norm="l2", stop_words="english", max_df=0.7)
X = vectorizer.fit_transform(df.body)

k = 5

kmeans = KMeans(n_clusters=k)
%time kmeans.fit(X)

df["cluster"] = kmeans.labels_
df[["text", "cluster"]]
```
`df["cluster"] = kmeans.labels_` ë¶€ë¶„ì€ ë°ì´í„° í”„ë ˆì„ì— `cluster`ë¼ëŠ” ìƒˆë¡œìš´ columì„ ë§Œë“¤ì–´ ì£¼ê³  í•´ë‹¹ raw ë°ì´í„°ì˜ cluster ë²ˆí˜¸ë¥¼ ì±„ì›Œì¤ë‹ˆë‹¤.  
\
ì•„ë˜ì˜ ì½”ë“œë¥¼ ì‹¤í–‰ì‹œì¼œ ë³´ë©´ ê° í´ëŸ¬ìŠ¤í„°ê°€ ì–¼ë§ˆë§Œí¼ì”© ìˆëŠ”ì§€ ì•Œë ¤ì¤ë‹ˆë‹¤.  
ë‹¹ì—°íˆ ë¶„í¬ê°€ ê³ ë¥´ë©´ ê³ ë¥¼ìˆ˜ë¡ ê· í˜•ì´ ì˜ ì¡í˜€ìˆëŠ” ì¢‹ì€ ê²°ê³¼ì…ë‹ˆë‹¤.  
```py
df.cluster.value_counts()
```

### LDA Topic Modeling  

ì´ 5ê°œì˜ í† í”½ìœ¼ë¡œ ë°ì´í„° í”„ë ˆì„ì˜ bodyë¥¼ ë¶„ë¥˜í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.  

```py
from sklearn.decomposition import LatentDirichletAllocation as LDA

num_topics = 5

lda = LDA(n_components=num_topics)
%time lda.fit(X)
```
\
LDAì˜ ê²°ê³¼ë¥¼ ë³´ê³  ì‹¶ë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ í•¨ìˆ˜ë¥¼ ì‹¤í–‰ì‹œì¼œ ì£¼ë©´ ë©ë‹ˆë‹¤.  
í•´ë‹¹ í•¨ìˆ˜ëŠ” í† í”½ ë‚´ ê° ë‹¨ì–´ë“¤ì˜ ê°€ì¤‘ì¹˜ë“¤ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê°€ì¤‘ì¹˜ê°€ ë†’ì„ìˆ˜ë¡ í•´ë‹¹ í† í”½ê³¼ ê¹Šì€ ì—°ê´€ì´ ìˆë‹¤ëŠ” ëœ»ì´ë©°, ì˜ ë¶„ë¥˜ë˜ì—ˆë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

```py
def show_topics(model, feature_names, num_top_words):
    for topic_idx, topic_scores in enumerate(model.components_):
        print("***Topic {}:".format(topic_idx))
        print(" + ".join(["{:.2f} * {}".format(topic_scores[i], feature_names[i]) for i in topic_scores.argsort()[::-1][:num_top_words]]))
        print()
```
\
ë§Œì•½ ì‹œê°í™”ë¥¼ í•˜ê³  ì‹¶ë‹¤ë©´, ë‹¤ìŒê³¼ ê°™ì´ `pyLDAvis`ë¥¼ ì‚¬ìš©í•´ ì£¼ë©´ ë©ë‹ˆë‹¤.  
```py
import pyLDAvis
import pyLDAvis.sklearn
pyLDAvis.enable_notebook()

pyLDAvis.sklearn.prepare(lda, X, vectorizer)
```
![4](./sw-coaching-week-5/4.png)  

## 3. ëŠë‚€ ì 
k-Means Clustering, Topic Modeling ë¶„ì„ì„ ì¨ë³´ë©´ì„œ ì„¸ìƒì—ëŠ” ì •ë§ë¡œ ë˜‘ë˜‘í•œ ì‚¬ëŒë“¤ì´ ë§ë‹¤ëŠ” ìƒê°ì„ í–ˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ ê°•ë ¥í•œ ë¶„ì„ì„ ê°„ë‹¨í•˜ê²Œ ì´ìš©í•  ìˆ˜ ìˆë„ë¡ ë¯¸ë¦¬ ë‹¤ ë§Œë“¤ì–´ ë‘ì—ˆë‹¤ëŠ” ì ì´ ëŒ€ë‹¨í–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ğŸ˜…   
ì´ë²ˆì—ëŠ” ê°„ë‹¨í•œ ì›ë¦¬ì™€ ê³¼ì •ë§Œ ì•Œì•„ë³´ê¸´ í–ˆì§€ë§Œ, ê·¸ë˜ë„ ì—¬ëŸ¬ ê°€ì§€ë¥¼ ë§ì´ ë°°ìš´ ëŠë‚Œì…ë‹ˆë‹¤. ë‚˜ì¤‘ì— ì¢€ ë” ìˆ˜í•™ì , ê³µí•™ì ìœ¼ë¡œ ê¹Šì€ ì´í•´ë¥¼ ìœ„í•´ ë…¸ë ¥í•´ì•¼ê² ìŠµë‹ˆë‹¤. ğŸ˜­ 

## Source
- ì„±ê· ê´€ëŒ€í•™êµ SW Coaching í”„ë¡œê·¸ë¨  
- ì´ê°•í‘œ(Kang-Pyo Lee) ë°•ì‚¬ë‹˜ ê°•ì˜  

<!--
1ì£¼ Web Scraping ê¸°ì´ˆ
2ì£¼ Web Scraping ì‹¬í™”
3ì£¼ Pandas Dataframe ë‹¤ë£¨ê¸°
4ì£¼ í…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬
5ì£¼ ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ë° í† í”½ ëª¨ë¸ë§
6ì£¼ ê°œì¸ í”„ë¡œì íŠ¸ ë°œí‘œ
-->